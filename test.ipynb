{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FAKE TEXT DETECTION BY TRAINING BERT ON GIVEN DATA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install pandas\n",
    "#%pip install numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LOAD DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fake = pd.read_csv(\"Data/Fake.csv/Fake.csv\")\n",
    "df_true = pd.read_csv(\"Data/True.csv/True.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EDIT DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fake[\"Label\"] = \"Fake\"\n",
    "df_true[\"Label\"] = \"True\"\n",
    "df = pd.concat([df_fake,df_true])\n",
    "df.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DETECT NULL VALUES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install matplotlib\n",
    "#%pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Figure\n",
    "fig, axes = plt.subplots(1,2, figsize = (15,6))\n",
    "#Adding the histogram1 - Fake News\n",
    "sns.histplot(df_fake.subject, palette = 'Set1', alpha = 0.5, ax = axes[0])\n",
    "axes[0].tick_params(axis = 'x', rotation = 90)\n",
    "axes[0].set_title('Fake News Subject')\n",
    "#Adding the histogram2 - True News\n",
    "sns.histplot(df_true.subject, palette = 'Set1', alpha = 0.5, ax = axes[1])\n",
    "axes[1].tick_params(axis = 'x', rotation = 90)\n",
    "axes[1].set_title('True  News Subject')\n",
    "#Printing the count of Subject\n",
    "print(\"Fake News Subject : \",dict(df_fake.subject.value_counts()))\n",
    "print(\"True News Subject : \",dict(df_true.subject.value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df.Label, palette = 'Set1', alpha = 0.5)\n",
    "plt.tick_params(axis = 'x', rotation = 90)\n",
    "plt.title('True VS Fake News')\n",
    "df.Label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SETUP TRAINING DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"] = df[\"title\"]+df[\"text\"] #considering text and title as X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Label'] = df['Label'].map({'True':1, 'Fake':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['Label'], stratify = df['Label'], test_size = 0.2, random_state = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SETTING UP BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SETUP AUTOTOKENIZER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "def tokenize(X):\n",
    "    X = tokenizer(\n",
    "        text = list(X),\n",
    "        add_special_tokens = True,\n",
    "        max_length = 100,\n",
    "        truncation = True,\n",
    "        padding = 'max_length',\n",
    "        return_tensors = 'tf',\n",
    "        return_token_type_ids = False,\n",
    "        return_attention_mask = True,\n",
    "        verbose = True\n",
    "        )\n",
    "    return X\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TOKENIZE VALUES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tokens = tokenize(X_train)\n",
    "X_test_tokens = tokenize(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test_tokens)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAKING  MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflow\n",
    "%pip install keras\n",
    "#%pip install tensorflow-cpu==2.10\n",
    "#%pip install tensorflow-directml-plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Dense, Dropout, Embedding\n",
    "Adams= tf.keras.optimizers.legacy.Adam\n",
    "from transformers import TFBertModel\n",
    "\n",
    "#print(tf.add([1.0, 2.0], [3.0, 4.0])) \n",
    "#tf.config.list_physical_devices('GPU') \n",
    "#print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CREATE BERT FUNCTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Length = 100\n",
    "def get_model():\n",
    "    dropout_rate = 0.2\n",
    "    input_ids = Input(shape = (Length,), dtype = tf.int32, name = 'input_ids')\n",
    "    input_mask = Input(shape = (Length,), dtype = tf.int32, name = 'input_mask')\n",
    "    embeddings = bert([input_ids, input_mask])[1] #pooler output\n",
    "    print(embeddings)\n",
    "    out = Dropout(0.2)(embeddings)\n",
    "    #64 units dense layer\n",
    "    out = Dense(64,activation = 'relu')(out)\n",
    "    out = Dropout(0.2)(out)\n",
    "    y = Dense(1,activation = 'sigmoid')(out)\n",
    "    model = Model(inputs=[input_ids, input_mask], outputs=y)\n",
    "    model.layers[2].trainable = True\n",
    "    #define optimizer\n",
    "    optimizer = Adams(learning_rate=1e-05, epsilon=1e-08, decay=0.01,clipnorm=1.0)\n",
    "    #complile the model\n",
    "    model.compile(optimizer = optimizer, loss = 'binary_crossentropy', metrics = 'accuracy')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = TFBertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PLOT MODEL STRUCTURE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install pydot\n",
    "#%pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "tf.keras.utils.plot_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "history = model.fit(x = {'input_ids':X_train_tokens['input_ids'],'input_mask':X_train_tokens['attention_mask']}, \n",
    "                    y = y_train, epochs=1, \n",
    "                    validation_split = 0.2, \n",
    "                    batch_size = 16, \n",
    "                    callbacks=[EarlyStopping( monitor='val_accuracy' ,mode='max', patience=3,verbose=False,restore_best_weights=True)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
